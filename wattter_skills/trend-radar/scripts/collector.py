import os
import sys
import json
import yaml
import asyncio
import argparse
import datetime
import re
from typing import List, Dict, Any
from collections import Counter
from playwright.async_api import async_playwright

# è·¯å¾„é…ç½®
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CONFIG_PATH = os.path.join(BASE_DIR, "config", "config.yaml")
KEYWORDS_PATH = os.path.join(BASE_DIR, "config", "keywords.txt")
REPORTS_DIR = os.path.join(BASE_DIR, "reports")

# ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
if not os.path.exists(REPORTS_DIR):
    os.makedirs(REPORTS_DIR)

def load_config() -> Dict[str, Any]:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    return {}

def load_default_keywords() -> List[str]:
    """ä» keywords.txt åŠ è½½é»˜è®¤å…³é”®è¯"""
    if os.path.exists(KEYWORDS_PATH):
        with open(KEYWORDS_PATH, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith("#")]
    return []

async def fetch_platform_data(context, p_id: str, platform_config: Dict[str, Any], crawler_config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ Playwright è·å–æŒ‡å®šå¹³å°çš„æ•°æ®"""
    url = platform_config.get("url")
    name = platform_config.get("name", p_id)
    
    page = await context.new_page()
    try:
        # éµå¾ª TrendRadar çš„åšæ³•ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
        # å°è¯•ç›´æ¥è®¿é—® APIï¼Œç”±äºæ˜¯åœ¨æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸­ï¼Œé€šå¸¸èƒ½ç»•è¿‡ç®€å•çš„ Cloudflare 403
        response = await page.goto(url, wait_until="domcontentloaded", timeout=crawler_config.get("timeout", 30) * 1000)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å– JSON
        content = await response.text()
        try:
            data = json.loads(content)
            items = []
            if isinstance(data, dict):
                # å…¼å®¹ newsnow çš„ä¸¤ç§æ ¼å¼: data å­—æ®µæˆ– items å­—æ®µ
                items = data.get("data", data.get("items", []))
            elif isinstance(data, list):
                items = data
            
            # æ ¼å¼åŒ–æ•°æ®ï¼Œç¡®ä¿åŒ…å«æ¥æºå¹³å°
            formatted_items = []
            for item in items:
                item["platform"] = name
                formatted_items.append(item)
            return formatted_items
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯ JSONï¼Œå¯èƒ½æ˜¯é‡åˆ°äº† HTML æŒ‘æˆ˜é¡µé¢æˆ– 403
            return []
    except Exception as e:
        print(f"Error fetching {name}: {e}", file=sys.stderr)
        return []
    finally:
        await page.close()

async def collect_all(target_platforms: List[str], config: Dict[str, Any]) -> Dict[str, Any]:
    """é‡‡é›†æ‰€æœ‰ç›®æ ‡å¹³å°çš„æ•°æ®"""
    crawler_cfg = config.get("crawler", {})
    platforms_cfg = config.get("platforms", {})
    
    results = {"data": [], "errors": {}}
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=crawler_cfg.get("headless", True))
        context = await browser.new_context(
            user_agent=crawler_cfg.get("user_agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        )
        
        # è®¿é—®ä¸»é¡µä»¥åˆå§‹åŒ–ä¼šè¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        try:
            page = await context.new_page()
            await page.goto("https://newsnow.busiyi.world/", wait_until="domcontentloaded", timeout=10000)
            await page.close()
        except:
            pass
            
        tasks = []
        for p_id in target_platforms:
            if p_id in platforms_cfg and platforms_cfg[p_id].get("enabled", True):
                tasks.append(fetch_platform_data(context, p_id, platforms_cfg[p_id], crawler_cfg))
        
        platform_results = await asyncio.gather(*tasks)
        for items in platform_results:
            results["data"].extend(items)
            
        await browser.close()
    return results

def filter_trends(trends: List[Dict[str, Any]], keywords: List[str], max_hours: int = 48) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å…³é”®è¯å’Œæ—¶æ•ˆæ€§ç­›é€‰çƒ­ç‚¹
    - max_hours: æœ€å¤§å°æ—¶æ•°ï¼ˆé»˜è®¤ 48 å°æ—¶ï¼‰
    """
    now = datetime.datetime.now()
    
    must_include = [kw[1:].lower() for kw in keywords if kw.startswith("+")]
    must_exclude = [kw[1:].lower() for kw in keywords if kw.startswith("!")]
    regular = [kw.lower() for kw in keywords if not kw.startswith("+") and not kw.startswith("!")]
    
    filtered = []
    for item in trends:
        # 1. æ—¶æ•ˆæ€§æ£€æŸ¥
        pub_date_str = item.get("pubDate")
        if pub_date_str:
            try:
                # å°è¯•è§£æå¤šç§å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%d"):
                    try:
                        pub_date = datetime.datetime.strptime(pub_date_str, fmt)
                        break
                    except:
                        continue
                else:
                    # å¦‚æœæ— æ³•è§£æï¼Œåˆ™è·³è¿‡æ—¶æ•ˆæ€§æ£€æŸ¥ï¼ˆæˆ–è§†ä¸ºé€šè¿‡ï¼Œå–å†³äºç­–ç•¥ï¼Œè¿™é‡Œé€‰æ‹©é€šè¿‡ä½†æ ‡è®°ï¼‰
                    pub_date = None
                
                if pub_date:
                    delta = now - pub_date
                    if delta.total_seconds() > max_hours * 3600:
                        continue # è¶…è¿‡ 48 å°æ—¶ï¼Œè·³è¿‡
            except:
                pass

        # 2. å…³é”®è¯ç­›é€‰
        title = item.get("title", item.get("text", item.get("name", ""))).lower()
        
        # æ£€æŸ¥æ’é™¤è¯
        if any(ex in title for ex in must_exclude):
            continue
            
        # æ£€æŸ¥å¿…é¡»åŒ…å«è¯
        if must_include and not all(inc in title for inc in must_include):
            continue
            
        # æ£€æŸ¥æ™®é€šè¯
        if regular:
            if any(reg in title for reg in regular):
                filtered.append(item)
        else:
            filtered.append(item)
            
    return filtered

def extract_topics(items: List[Dict[str, Any]], top_n=5) -> List[tuple]:
    """æå–å…³é”®è¯é¢‘ç‡ï¼Œæ¨¡æ‹Ÿçƒ­ç‚¹è¯é¢˜èšåˆ"""
    titles = [item.get("title", item.get("text", "")) for item in items]
    text = " ".join(titles)
    # ç®€å•çš„åˆ†è¯é€»è¾‘ï¼ˆé’ˆå¯¹ä¸­æ–‡/è‹±æ–‡æ··åˆï¼‰
    words = re.findall(r'[\u4e00-\u9fa5]{2,}|[a-zA-Z]{2,}', text)
    # æ’é™¤å¸¸è§çš„æ— æ„ä¹‰è¯ï¼ˆåœç”¨è¯ï¼‰
    stop_words = {"è¿™ä¸ª", "ä»€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "å‘å¸ƒ", "æ­£å¼", "åº”ç”¨", "æ¨å‡º", "è¿›è¡Œ"}
    words = [w for w in words if w not in stop_words]
    return Counter(words).most_common(top_n)

def save_as_markdown(data: Dict[str, Any], custom_path: str = None):
    """å°†ç»“æœä¿å­˜ä¸ºæ›´ç²¾ç¾çš„ Markdown æŠ¥å‘Š"""
    if custom_path:
        filepath = os.path.abspath(custom_path)
        # å¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œåˆ™åœ¨è¯¥ç›®å½•ä¸‹ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
        if os.path.isdir(filepath):
            now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(filepath, f"trend_report_{now}.md")
        # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
    else:
        now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"trend_report_{now}.md"
        filepath = os.path.join(REPORTS_DIR, filename)
    
    # æå–æ ¸å¿ƒè¯é¢˜
    topics = extract_topics(data["items"])
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# ğŸš€ TrendRadar å…¨ç½‘çƒ­ç‚¹ç›‘æ§æŠ¥å‘Š\n\n")
        
        f.write(f"## ğŸ“Š ç›‘æ§æ¦‚è§ˆ\n")
        f.write(f"| é¡¹ç›® | å†…å®¹ |\n")
        f.write(f"| :--- | :--- |\n")
        f.write(f"| **ç”Ÿæˆæ—¶é—´** | {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |\n")
        f.write(f"| **ç›‘æ§å¹³å°** | {', '.join(data['platforms'])} |\n")
        f.write(f"| **ç­›é€‰å…³é”®è¯** | `{', '.join(data['keywords']) if data['keywords'] else 'å…¨ç½‘çƒ­æœ'}` |\n")
        f.write(f"| **å‘½ä¸­æ•°é‡** | {data['count']} |\n\n")
        
        if topics:
            f.write(f"## ğŸ’¡ æ ¸å¿ƒè¯é¢˜è¯äº‘ (æ¨¡æ‹Ÿåˆ†æ)\n")
            f.write("> æ ¹æ®å½“å‰é‡‡é›†åˆ°çš„æ ‡é¢˜ï¼Œä¸ºæ‚¨æå–å‡ºç°é¢‘ç‡æœ€é«˜çš„è¯é¢˜è¯ï¼š\n\n")
            topic_str = "  ".join([f"`{t[0]}({t[1]})`" for t in topics])
            f.write(f"{topic_str}\n\n")

        f.write(f"## ğŸ” å®æ—¶çƒ­ç‚¹åˆ—è¡¨ (48hå†…)\n\n")
        f.write(f"| æ’å | å¹³å° | æ ‡é¢˜ | æ—¶é—´ | çƒ­åº¦ | é“¾æ¥ |\n")
        f.write(f"| :--- | :--- | :--- | :--- | :--- | :--- |\n")
        
        for i, item in enumerate(data["items"], 1):
            title = item.get("title", item.get("text", "æ— æ ‡é¢˜")).replace("|", "\\|")
            platform = item.get("platform", "æœªçŸ¥")
            url = item.get("url", item.get("mobileUrl", "#"))
            pub_date = item.get("pubDate", "-")
            
            # æå–çƒ­åº¦å€¼
            hot = item.get("hot", item.get("heat", item.get("score", "-")))
            if isinstance(hot, (int, float)) and hot > 10000:
                hot = f"{hot/10000:.1f}ä¸‡"
            
            f.write(f"| {i} | {platform} | {title} | {pub_date} | {hot} | [æŸ¥çœ‹è¯¦æƒ…]({url}) |\n")
        
        f.write(f"\n----- \n")
        f.write(f"*ğŸ’¡ æç¤ºï¼šæœ¬æŠ¥å‘Šç”± TrendRadar è‡ªåŠ¨ç”Ÿæˆã€‚å¦‚éœ€ AI æ·±åº¦åˆ†æï¼ˆåŒ…æ‹¬è¶‹åŠ¿é¢„æµ‹ã€æƒ…æ„Ÿæ´å¯Ÿç­‰ï¼‰ï¼Œè¯·ç›´æ¥åœ¨å¯¹è¯æ¡†ä¸­è¦æ±‚æˆ‘è¿›è¡Œåˆ†æã€‚*\n")
            
    return filepath

async def main():
    parser = argparse.ArgumentParser(description="TrendRadar Robust Collector (Playwright Edition)")
    parser.add_argument("--keywords", type=str, help="Comma separated keywords to filter trends")
    parser.add_argument("--platforms", type=str, help="Comma separated platform IDs to fetch from")
    parser.add_argument("--output", type=str, help="Custom path to save the markdown report")
    parser.add_argument("--hours", type=int, default=48, help="Max hours since publication (default: 48)")
    
    args = parser.parse_args()
    
    config = load_config()
    
    # ç¡®å®šè¦æœç´¢çš„å…³é”®è¯ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œä¸å¼ºåˆ¶ä¾èµ–é…ç½®æ–‡ä»¶
    if args.keywords:
        target_keywords = [kw.strip() for kw in args.keywords.split(",")]
    else:
        # å¦‚æœæ²¡æœ‰ä¼ å…¥å…³é”®è¯ï¼Œå°è¯•åŠ è½½é»˜è®¤æ–‡ä»¶ï¼Œä½†ä¸å†å¼ºåˆ¶è¦æ±‚åœ¨ config.yaml ä¸­é…ç½®
        target_keywords = load_default_keywords()
        
    # ç¡®å®šè¦æœç´¢çš„å¹³å°
    platforms_cfg = config.get("platforms", {})
    if args.platforms:
        target_platforms = [p.strip() for p in args.platforms.split(",")]
    else:
        target_platforms = [p for p, cfg in platforms_cfg.items() if cfg.get("enabled", True)]
    
    # æ‰§è¡Œé‡‡é›†
    results = await collect_all(target_platforms, config)
    
    # æ‰§è¡Œç­›é€‰
    filtered_data = filter_trends(results["data"], target_keywords, max_hours=args.hours)
    
    # æŒ‰ç…§çƒ­åº¦æ’åº
    def get_hot(item):
        hot = item.get("hot", item.get("heat", item.get("score", 0)))
        if isinstance(hot, str):
            if "ä¸‡" in hot:
                try: return float(hot.replace("ä¸‡", "")) * 10000
                except: return 0
        try: return float(hot)
        except: return 0

    filtered_data.sort(key=get_hot, reverse=True)
    
    # è¾“å‡ºç»“æœï¼ˆJSON æ ¼å¼ä¾› AI è§£æï¼‰
    max_items = config.get("report", {}).get("max_items_per_platform", 20)
    output = {
        "count": len(filtered_data),
        "platforms": target_platforms,
        "keywords": target_keywords,
        "items": filtered_data[:max_items * len(target_platforms)]
    }
    
    # ä¿å­˜ä¸º Markdownï¼ˆæ”¯æŒè‡ªå®šä¹‰è·¯å¾„ï¼‰
    report_path = save_as_markdown(output, custom_path=args.output)
    output["report_path"] = report_path
    
    print(json.dumps(output, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
